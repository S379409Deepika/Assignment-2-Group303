{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcRawueZfzH1"
      },
      "source": [
        "## **Task** 1: Extract 'text' from CSV files and store them into a single .txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UK4DNPzrbLco"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of CSV files\n",
        "csv_files = ['CSV1.csv', 'CSV2.csv', 'CSV3.csv', 'CSV4.csv']\n",
        "\n",
        "# Open a new text file to store extracted texts\n",
        "with open('all_texts.txt', 'w') as outfile:\n",
        "    for file in csv_files:\n",
        "        # Load each CSV file\n",
        "        df = pd.read_csv(file)\n",
        "\n",
        "        # Assuming the 'text' column contains the large text we need\n",
        "        texts = df['TEXT'].tolist()\n",
        "\n",
        "        # Write all text into a single file\n",
        "        for text in texts:\n",
        "            outfile.write(text + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsKzN7GOgjdq"
      },
      "source": [
        "## Task 2: Install and Set Up NLP **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyevXMGSfyDu",
        "outputId": "99a5b173-bd55-4729-9e45-1cc0e30d5d89"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install spacy scispacy transformers torch\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_ner_bc5cdr_md-0.3.0.tar.gz\n",
        "\n",
        "# Download BioBERT model from Hugging Face\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdkIF1SPh8xD"
      },
      "source": [
        "## **Task** 3: Count Word Occurrences and Unique Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOMAONe2iJkK"
      },
      "source": [
        "3.1: Count Occurrences of Words and Store Top 30 Words in CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u4GSv-AwnHTc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1x_8hICIiOGS"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Load the combined text file\n",
        "with open('all_texts.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text and count word occurrences\n",
        "words = text.split()\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get the top 30 most common words\n",
        "top_30_words = word_counts.most_common(30)\n",
        "\n",
        "# Save the top 30 words and their counts into a CSV file\n",
        "top_words_df = pd.DataFrame(top_30_words, columns=['Word', 'Count'])\n",
        "top_words_df.to_csv('top_30_words.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6s0_YeCjIpC"
      },
      "source": [
        "3.2: Use Auto Tokenizer to Count Unique Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBX0H_IpjJkx",
        "outputId": "7e0aa4d9-3af7-403c-c0c2-e6f2583651a5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from collections import Counter\n",
        "\n",
        "# Load the text from the 'all_texts.txt' file\n",
        "with open('all_texts.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Load the BioBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "def count_unique_tokens(text):\n",
        "    # Tokenize the text using the BioBERT tokenizer\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Count the occurrences of each token\n",
        "    token_counts = Counter(tokens)\n",
        "\n",
        "    # Get the top 30 unique tokens\n",
        "    top_30_tokens = token_counts.most_common(30)\n",
        "\n",
        "    return top_30_tokens\n",
        "\n",
        "# Get unique token counts\n",
        "top_30_tokens = count_unique_tokens(text)\n",
        "\n",
        "# Print the top 30 tokens and their counts\n",
        "for token, count in top_30_tokens:\n",
        "    print(f\"Token: {token}, Count: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiqdMDpGjOcj"
      },
      "source": [
        "## **Task** 4: Named-Entity Recognition (NER) Using SpaCy, SciSpaCy, and BioBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95kPyo0dkGnV"
      },
      "source": [
        "Using SpaCy and SciSpaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "O33gXN-hjV_i",
        "outputId": "b9760ccb-3209-4c5c-c227-03795ef7faef"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the SpaCy models for NER\n",
        "nlp_sci_sm = spacy.load(\"en_core_sci_sm\")\n",
        "nlp_bc5cdr_md = spacy.load(\"en_ner_bc5cdr_md\")\n",
        "\n",
        "def extract_entities(text, nlp_model):\n",
        "    doc = nlp_model(text)\n",
        "    diseases = [ent.text for ent in doc.ents if ent.label_ == \"DISEASE\"]\n",
        "    drugs = [ent.text for ent in doc.ents if ent.label_ == \"DRUG\"]\n",
        "    return diseases, drugs\n",
        "\n",
        "# Load the text from 'all_texts.txt'\n",
        "with open('all_texts.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Extract entities using en_core_sci_sm\n",
        "diseases_sci_sm, drugs_sci_sm = extract_entities(text, nlp_sci_sm)\n",
        "\n",
        "# Extract entities using en_ner_bc5cdr_md\n",
        "diseases_bc5cdr, drugs_bc5cdr = extract_entities(text, nlp_bc5cdr_md)\n",
        "\n",
        "# Compare entity counts\n",
        "print(f\"Total diseases detected by en_core_sci_sm: {len(diseases_sci_sm)}\")\n",
        "print(f\"Total drugs detected by en_core_sci_sm: {len(drugs_sci_sm)}\")\n",
        "print(f\"Total diseases detected by en_ner_bc5cdr_md: {len(diseases_bc5cdr)}\")\n",
        "print(f\"Total drugs detected by en_ner_bc5cdr_md: {len(drugs_bc5cdr)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OKG94tVkPU6"
      },
      "source": [
        "Using BioBERT with Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrrD9l76kQVu"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the BioBERT model for NER\n",
        "nlp_biobert = pipeline(\"ner\", model=\"dmis-lab/biobert-base-cased-v1.1\", tokenizer=\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "# Apply the NER pipeline to the text\n",
        "ner_results = nlp_biobert(text)\n",
        "\n",
        "# Filter for disease and drug entities\n",
        "diseases_biobert = [ent['word'] for ent in ner_results if 'disease' in ent['entity'].lower()]\n",
        "drugs_biobert = [ent['word'] for ent in ner_results if 'drug' in ent['entity'].lower()]\n",
        "\n",
        "print(f\"Total diseases detected by BioBERT: {len(diseases_biobert)}\")\n",
        "print(f\"Total drugs detected by BioBERT: {len(drugs_biobert)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEYIc-Ekh-7"
      },
      "source": [
        "Comparison of Models\n",
        "::\n",
        "To compare the models, we can look at the total number of entities detected, check for common words, and note the differences in performance or entity types:bold text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuXsIb5Tkqre"
      },
      "outputs": [],
      "source": [
        "# Compare detected entities across models\n",
        "print(f\"Diseases - SpaCy en_core_sci_sm: {len(diseases_sci_sm)}\")\n",
        "print(f\"Diseases - SciSpaCy en_ner_bc5cdr_md: {len(diseases_bc5cdr)}\")\n",
        "print(f\"Diseases - BioBERT: {len(diseases_biobert)}\")\n",
        "\n",
        "print(f\"Drugs - SpaCy en_core_sci_sm: {len(drugs_sci_sm)}\")\n",
        "print(f\"Drugs - SciSpaCy en_ner_bc5cdr_md: {len(drugs_bc5cdr)}\")\n",
        "print(f\"Drugs - BioBERT: {len(drugs_biobert)}\")\n",
        "\n",
        "# Check for common diseases and drugs between models\n",
        "common_diseases = set(diseases_sci_sm).intersection(set(diseases_bc5cdr), set(diseases_biobert))\n",
        "common_drugs = set(drugs_sci_sm).intersection(set(drugs_bc5cdr), set(drugs_biobert))\n",
        "\n",
        "print(f\"Common diseases across models: {common_diseases}\")\n",
        "print(f\"Common drugs across models: {common_drugs}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
